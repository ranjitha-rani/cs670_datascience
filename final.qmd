---
title: "CS 670: Data Science"
subtitle: "Network Traffic Analysis for Intrusion Detection"
author: "Ranjitha Rani"
format: html
---

## Introduction

The rise in global cyber threats has heightened the importance of intrusion detection systems in modern network security. With organizations generating massive volumes of network traffic data, developing effective data-driven intrusion detection strategies has become increasingly critical. The UNSW-NB15 dataset is one such comprehensive resource that simulates real-world network traffic, including both normal activity and a wide range of synthetic attacks. 



## 1. Data Selection and Overview

**Source:** Australian Centre for Cyber Security (ACCS), UNSW Canberra  
**Dataset:** UNSW-NB15 Public Dataset

**Creator & Purpose:**  
The dataset was generated by the IXIA PerfectStorm tool in 2015, simulating normal user behavior along with a variety of modern synthetic attack types. It includes raw traffic features and labelled records for supervised learning and analysis of cyber-attacks.

**Attributes & Description:**  
The dataset includes over 2 million network flow records distributed over four CSV files. Each record contains 49 attributes representing:

- **Basic connection features**  
  Source/destination IP, port, protocol

- **Flow features**  
  Number of packets, bytes, time duration, direction

- **Content features**  
  Login attempts, HTTP methods, service requests

- **Labelled fields**  
  `label` (binary), `attack_cat` (multiclass attack type)


**Challenges and Considerations:**  
> Some features like `is_ftp_login` and `ct_ftp_cmd` require special handling.  
> Mixed data types and sparsely populated fields posed representation challenges.  


This dataset serves as a strong foundation for detailed exploratory analysis, interactive visualizations, and classification modeling to detect network intrusions.


## Import Libraries {.unnumbered}

This section loads all essential libraries used for preprocessing, modeling (classification & regression), interactive visualizations, and evaluation.  

```{python}
#| code-fold: true
#| code-summary: "Click to show/hide library imports"


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import plotly.express as px
import plotly.graph_objects as go
import statsmodels.api as sm
from scipy import stats
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report, average_precision_score,
    precision_recall_curve, roc_curve
)
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from pandas.plotting import scatter_matrix
import warnings
warnings.filterwarnings("ignore")
```

## 2. Data Preprocessing

> The dataset is available in four csv files.

### 2.1 Merging Dataset using features.csv

```{python}
#| code-fold: true
#| code-summary: "Show Code"


df1 = pd.read_csv("UNSW-NB15_1.csv", encoding="ISO-8859-1")
df2 = pd.read_csv("UNSW-NB15_2.csv", encoding="ISO-8859-1")
df3 = pd.read_csv("UNSW-NB15_3.csv", encoding="ISO-8859-1")
df4 = pd.read_csv("UNSW-NB15_4.csv", encoding="ISO-8859-1")
df_features = pd.read_csv("NUSW-NB15_features.csv", encoding="ISO-8859-1")


df1.columns = df_features['Name']
df2.columns = df_features['Name']
df3.columns = df_features['Name']
df4.columns = df_features['Name']

print('Features:')
display(df_features.head())

merged_df=pd.concat([df1,df2,df3,df4],ignore_index=True)
print('Merged Dataset:')
display(merged_df.head())
print("dataset shape: ",merged_df.shape)

```
### 2.2 Identifying Numerical and Categorical Columns

```{python}
#| code-fold: true
#| code-summary: "Show Code"

numerical_columns = merged_df.select_dtypes(include=['number']).columns
categorical_columns = merged_df.select_dtypes(exclude=['number']).columns

print(f"There are {len(numerical_columns)} Numerical Columns in the dataset:")
print(list(numerical_columns)) 

print(f"\nThere are {len(categorical_columns)} Categorical Columns in the dataset:")
print(list(categorical_columns)) 

```

### 2.3 Duplicate Handling

```{python}
#| code-fold: true
#| code-summary: "Show Code"


duplicate_count = merged_df.duplicated().sum()
print(f"Duplicates in dataset: {duplicate_count}")


total_rows_before = merged_df.shape[0]


merged_df = merged_df.drop_duplicates()

total_rows_after = merged_df.shape[0]
print(f"New dataset shape after removing duplicates: {merged_df.shape}")



```

### 2.4 Missing Data Handling

```{python}
#| code-fold: true
#| code-summary: "Show Code"

missing_data = merged_df.isnull().sum().reset_index()
missing_data.columns = ["Column Name", "Total Missing Values"]
missing_data["% Missing"] = (missing_data["Total Missing Values"] / len(merged_df) * 100).round(2)


missing_data = missing_data[missing_data["Total Missing Values"] > 0].sort_values(by="% Missing", ascending=False)


missing_data
```


### 2.4.1 Handling 'attack_cat'
To ensure consistency :

-**Handled Missing Values** â€“ Assigned "unknown_attack" to missing attack labels where label = 1 and "normal" otherwise.
-**Standardized Attack Names** â€“ Trimmed spaces, converted to lowercase, and renamed ambiguous categories (e.g., "dos" â†’ "denial_of_service").
-**Encoded Categories**â€“ Converted attack labels into numerical values using Label Encoding for machine learning compatibility.

```{python}
#| code-fold: true
#| code-summary: "Show code"

if 'label' in merged_df.columns:
    merged_df.loc[(merged_df['attack_cat'].isna()) & (merged_df['label'] == 1), 'attack_cat'] = 'unknown_attack'

merged_df['attack_cat'].fillna('normal', inplace=True)

merged_df['attack_cat'] = merged_df['attack_cat'].str.strip().str.lower()

attack_mapping = {
    'dos': 'denial_of_service',
    'ddos': 'denial_of_service',
    'worm': 'worm_attack',
    'backdoor': 'backdoor_attack',
    'recon': 'reconnaissance',
    'fuzzers': 'fuzzing_attack',
}


merged_df['attack_cat'] = merged_df['attack_cat'].replace(attack_mapping)


le = LabelEncoder()
merged_df['attack_cat_encoded'] = le.fit_transform(merged_df['attack_cat'])


print("Unique values in attack_cat after processing:")
print(merged_df['attack_cat'].value_counts())

print("\nEncoded values:")
print(merged_df[['attack_cat', 'attack_cat_encoded']].drop_duplicates())


```

### 2.4.2 'Handling ct_flw_http_mthd'

To ensure data consistency, we impute missing values in ct_flw_http_mthd (HTTP flow methods) using a data-driven approach:

-> Identified Mode & Median â€“ The most frequent (mode) and middle value (median) were computed.
Smart Imputation â€“ ->If the mode is 0 and appears frequently, we assign 0; otherwise, we use the median to fill missing values.
This method balances preserving common trends while preventing bias from extreme values.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

http_mode = merged_df['ct_flw_http_mthd'].mode()[0]


http_median = merged_df['ct_flw_http_mthd'].median()


fill_value = 0 if http_mode == 0 else http_median


merged_df['ct_flw_http_mthd'].fillna(fill_value, inplace=True)


print(f"Filling missing values with: {fill_value}")
print(merged_df['ct_flw_http_mthd'].describe())
```

### 2.4.3 'Handling is_ftp_login'

To maintain accuracy while filling missing values in is_ftp_login, :

1.Group by Protocol (proto) â€“ Since FTP-related logins depend on protocol type, missing values are filled using the most frequent (mode) value within each protocol group.
2.If no mode exists for a group, we assign 0 (assuming no FTP login).
3.The column remains in integer format for model compatibility.

```{python}
#| code-fold: true
#| code-summary: "Show Code"


merged_df['is_ftp_login'] = merged_df['is_ftp_login'].fillna(
    merged_df.groupby('proto')['is_ftp_login'].transform(lambda x: x.mode()[0] if not x.mode().empty else 0)
)

def convert_to_binary(df, column):
    df[column] = df[column].fillna(0)  
    df[column] = (df[column] > 0).astype(int)
    return df

convert_to_binary(merged_df, 'is_ftp_login')

print(merged_df['is_ftp_login'].value_counts())


```

### 2.4.4 'Handling ct_ftp_cmd'


Some rows in the `ct_ftp_cmd` column contained empty strings instead of numeric values. To ensure consistency and avoid errors during modeling, replace blank entries with `'0'` and convert the column to integer type.

```{python}
#| code-fold: true
#| code-summary: "Show Code"


merged_df['ct_ftp_cmd'] = merged_df['ct_ftp_cmd'].replace(' ', '0').fillna(0).astype(int)
```

### 2.4.5 'Handling sport and dsport'


```{python}
merged_df['sport'] = pd.to_numeric(merged_df['sport'], errors='coerce').astype('Int64')
merged_df['dsport'] = pd.to_numeric(merged_df['dsport'], errors='coerce').astype('Int64')
```

## 3. Exploratory Data analysis

### 3.1 Attack Categorization

```{python}
#| code-fold: true
#| code-summary: "Show Code"

if "attack_cat" in merged_df.columns:
    attack_counts = merged_df["attack_cat"].value_counts().reset_index()
    attack_counts.columns = ["Attack Category", "Count"]

    
    fig = px.pie(
        attack_counts,
        names="Attack Category",
        values="Count",
        title="Attack Categories Distribution",
        hole=0.4, 
        color="Attack Category"
    )

    fig.show()
else:
    print("Column 'attack_cat' not found in the dataset!")
```

### 3.2 Protocol Usage Across Attacks

```{python}
#| code-fold: true
#| code-summary: "Show Code"

if "proto" in merged_df.columns and "attack_cat" in merged_df.columns:
  
    proto_counts = merged_df.groupby("proto")["attack_cat"].count().reset_index()
    proto_counts.columns = ["Protocol", "Count"]

   
    fig = px.bar(
        proto_counts,
        x="Protocol",
        y="Count",
        text="Count",
        title="Protocol Usage Across Attacks",
        labels={"Protocol": "Network Protocol", "Count": "Number of Occurrences"},
        color="Count",
        color_continuous_scale="Teal",
    )

 
    fig.update_traces(texttemplate='%{text}', textposition='outside')
    fig.update_layout(xaxis_tickangle=-45, height=600)

   
    fig.show()
else:
    print("Columns 'proto' or 'attack_cat' not found in the dataset!")
```

### 3.3 Source vs Destination Port Mapping

Attack patterns show consistent usage of high-risk ports.



```{python}
#| code-fold: true
#| code-summary: "Show Code"

if "sport" in merged_df.columns and "dsport" in merged_df.columns:
    import plotly.express as px

    
    top_sport = merged_df["sport"].value_counts().nlargest(15).reset_index()
    top_sport.columns = ["Source Port", "Count"]


    
    fig3 = px.scatter(
        merged_df.sample(5000),  
        x="sport",
        y="dsport",
        title="Source vs Destination Ports Mapping",
        labels={"sport": "Source Port", "dsport": "Destination Port"},
        color="dsport",
        color_continuous_scale="Viridis",
        opacity=0.6,
    )
    fig3.update_layout(height=600)



    fig3.show()

else:
    print("Columns 'sport' or 'dsport' not found in the dataset!")
```


### 3.4 Correlation Heatmap

```{python}

#| code-fold: true
#| code-summary: "Show Code"




numeric_df = merged_df.select_dtypes(include=['int64', 'float64'])


corr_matrix = numeric_df.corr()


plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()


```


### 3.5 Pairplots

Identifying patterns, correlations, or separations between features for different attack categories.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

sampled_df = merged_df.sample(n=25000, random_state=43)

selected_features = ['ct_dst_sport_ltm', 'ct_src_ ltm', 'ct_state_ttl', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'attack_cat_encoded']


sns.pairplot(sampled_df[selected_features], hue='attack_cat_encoded', palette='deep', corner=True)
plt.suptitle("Pairplot of Selected Features by Attack Category", y=1.02)
plt.show()
```

### 3.6 Feature Distribution - Numerical 

Frequency distribution of a numerical feature using a histogram, overlaid with a Kernel Density Estimate (KDE) curve.
The histogram represents the count of data points within specific intervals (bins).
The KDE curve provides a smoothed estimate of the feature's probability density, helping to visualize the overall shape of the distribution.


```{python}
#| code-fold: true
#| code-summary: "Show Code"


sampled_df = merged_df.sample(n=20000, random_state=43)


dist_features = ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss']


for feature in dist_features:
    plt.figure(figsize=(8, 4))
    sns.histplot(sampled_df[feature], kde=True, bins=30, color='lightcoral')
    plt.title(f"Distribution of '{feature}'")
    plt.xlabel(feature)
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()
```


### 3.7 Outliers

```{python}
#| code-fold: true
#| code-summary: "Show Code"

plt.figure(figsize=(15, 6))
merged_df[numerical_columns].boxplot(rot=90)
plt.title("Boxplot of Numerical Columns (Checking Outliers)")
plt.savefig("Boxplot of Numerical Columns (Checking Outliers).png")
plt.show()
```


### 3.8 Statistical Analysis

Filtering out the columns to be excluding after the statistical analysis and outlier analysation.

```{python}
#| code-fold: true
#| code-summary: "Show Code"

numerical_columns = merged_df.select_dtypes(include=['float64', 'int64']).columns.tolist()


exclude_columns = ['sport', 'swim', 'dwim', 'stcpb', 'dtcpb', 'Stime', 'Ltime']


numerical_columns = [col for col in numerical_columns if col not in exclude_columns]


for col in numerical_columns:
    median_value = merged_df[col].median()  # Getting the median value
    lower_bound = merged_df[col].quantile(0.25) - 1.5 * (merged_df[col].quantile(0.75) - merged_df[col].quantile(0.25))  # Lower bound for outliers
    upper_bound = merged_df[col].quantile(0.75) + 1.5 * (merged_df[col].quantile(0.75) - merged_df[col].quantile(0.25))  # Upper bound for outliers

    # Replace outliers with the median value
    merged_df[col] = merged_df[col].apply(lambda x: median_value if x < lower_bound or x > upper_bound else x)


summary_stats = merged_df[numerical_columns].describe().transpose()


display(summary_stats)
```

## 4. Feature Engineering and selection

Engineering the numerical values for selecting the best features for the modelling.
```{python}
#| code-fold: true
#| code-summary: "Show Code"

def generate_features(df):
    # Duration
    df['duration'] = df['Ltime'] - df['Stime']

    # Ratios
    df['byte_ratio'] = df['sbytes'] / (df['dbytes'] + 1)
    df['pkt_ratio'] = df['Spkts'] / (df['Dpkts'] + 1)
    df['load_ratio'] = df['Sload'] / (df['Dload'] + 1)
    df['jit_ratio'] = df['Sjit'] / (df['Djit'] + 1)
    df['inter_pkt_ratio'] = df['Sintpkt'] / (df['Dintpkt'] + 1)
    df['tcp_setup_ratio'] = df['tcprtt'] / (df['synack'] + df['ackdat'] + 1)

    # Aggregate Features
    df['total_bytes'] = df['sbytes'] + df['dbytes']
    df['total_pkts'] = df['Spkts'] + df['Dpkts']
    df['total_load'] = df['Sload'] + df['Dload']
    df['total_jitter'] = df['Sjit'] + df['Djit']
    df['total_inter_pkt'] = df['Sintpkt'] + df['Dintpkt']
    df['total_tcp_setup'] = df['tcprtt'] + df['synack'] + df['ackdat']

    # Interaction Features
    df['byte_pkt_interaction_src'] = df['sbytes'] * df['Spkts']
    df['byte_pkt_interaction_dst'] = df['dbytes'] * df['Dpkts']
    df['load_jit_interaction_src'] = df['Sload'] * df['Sjit']
    df['load_jit_interaction_dst'] = df['Dload'] * df['Djit']
    df['pkt_jit_interaction_src'] = df['Spkts'] * df['Sjit']
    df['pkt_jit_interaction_dst'] = df['Dpkts'] * df['Djit']

    # Statistical Features
    df['mean_pkt_size'] = df['smeansz'] + df['dmeansz']
    df['tcp_seq_diff'] = df['stcpb'] - df['dtcpb']

    return df


generate_features(merged_df)


columns_to_drop = ['sport', 'dsport', 'proto','srcip', 'dstip','state', 'service']
merged_df.drop(columns=columns_to_drop, inplace=True)
```


Encoding the attack categories
```{python}
cat_columns = merged_df.select_dtypes(include=['O']).columns.tolist()
cat_columns


from sklearn.preprocessing import OneHotEncoder

label_encoder = LabelEncoder()
ohe = OneHotEncoder()

merged_df['attack_cat'] = label_encoder.fit_transform(merged_df['attack_cat'])

label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print("Label Mapping:")
print(label_mapping)
```


## 4.1 Correlation 

```{python}
#| code-fold: true
#| code-summary: "Show Code"

plt.figure(figsize=(40,20))
plt.title("Correlation Plot")
sns.heatmap(merged_df.corr(),cmap='YlGnBu')


correlation_matrix = merged_df.corr()


high_correlation_mask = correlation_matrix >= 0.75


highly_correlated_features = []

for feature in high_correlation_mask.columns:
    correlated_with = high_correlation_mask.index[high_correlation_mask[feature]].tolist()
    for correlated_feature in correlated_with:
        if feature != correlated_feature and (correlated_feature, feature) not in highly_correlated_features:
            highly_correlated_features.append((feature, correlated_feature))


print("Highly correlated features:")
for feature1, feature2 in highly_correlated_features:
    print(f"{feature1} and {feature2}")
```

## 4.2 SMOTE - Synthetic Minority Over-sampling Technique

Imbalanced datasets impact the performance of the machine learning models. Addresses the class imbalance problem by generating synthetic samples for the minority class. 
```{python}
#| code-fold: true
#| code-summary: "Show Code"

features_to_drop = set()


for feature1, feature2 in highly_correlated_features:
    if feature1 not in features_to_drop and feature2 not in features_to_drop:
        features_to_drop.add(feature2)  # You can choose feature1 or feature2 to drop

train_df = merged_df.drop(columns=features_to_drop)


print("Remaining features after dropping highly correlated ones:")
print(train_df.columns)

x = train_df.drop(['attack_cat'], axis=1)
y = train_df[['attack_cat']]



desired_count = 15000


%pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.pipeline import Pipeline


oversample_strategy = {i: desired_count for i in range(len(y.value_counts())) if y.value_counts()[i] < desired_count}


undersample_strategy = {i: desired_count for i in range(len(y.value_counts())) if y.value_counts()[i] > desired_count}


smote = SMOTE(sampling_strategy=oversample_strategy)
undersample = RandomUnderSampler(sampling_strategy=undersample_strategy)


x_smote, y_smote = smote.fit_resample(x, y)


x_resampled, y_resampled = undersample.fit_resample(x_smote, y_smote)


print("Before resampling:")
print(y.value_counts())
print("\nAfter resampling:")
print(y_resampled.value_counts())


x = x_resampled
y = y_resampled



from sklearn.feature_selection import mutual_info_regression

discrete_features = x.dtypes == int

def mi_score_maker(x,y,discrete_features):
    scores = mutual_info_regression(x,y,discrete_features=discrete_features)
    df = pd.DataFrame({
        'Features':x.columns,
        'Scores':scores
    })
    df = df.sort_values(['Scores'],ascending=False).reset_index(drop=True)
    return df


mi_scores = mi_score_maker(x,y.astype('float64'),discrete_features)

mi_scores
```


## 4.3 Mutual information scores plot
```{python}

plt.figure(figsize=(10, 8))


sns.barplot(x='Scores', y='Features', data=mi_scores)


plt.title("Mutual Information Scores", fontsize=16)


plt.yticks(rotation=0)


plt.xticks(rotation=45)


plt.tight_layout()  
plt.savefig("Mutual Information Scores.png")
plt.show()

```


- **Selected Features**: byte_ratio,sbytes,smeansz,load_ratio, dbytes,pkt_ratio,duration


## 5. Modelling

## 5.1 Scaling Features
```{python}

#| code-fold: true
#| code-summary: "Show Code"

selected_features = ["byte_ratio", "sbytes", "smeansz", "load_ratio", 
                     "dbytes", "pkt_ratio", "duration"]


X_selected = x_resampled[selected_features]  # Resampled X with selected features
y_selected = y_resampled  # Target variable remains the same


print("Selected Features (X):\n", X_selected.head())
print("\nTarget Variable (y):\n", y_selected.head())

```

```{python}
#| code-fold: true
#| code-summary: "Show Code"

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(X_selected, y_selected, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler


scaler = StandardScaler()


scaler.fit(x_train)


x_train_scaled = scaler.transform(x_train)
x_test_scaled = scaler.transform(x_test)


x_train_scaled_df = pd.DataFrame(x_train_scaled, columns=x_train.columns)
x_test_scaled_df = pd.DataFrame(x_test_scaled, columns=x_test.columns)


print("Scaled Training Features:\n", x_train_scaled_df.head())
print("Scaled Testing Features:\n", x_test_scaled_df.head())
```



## 5.2 Model -1 : KNN(Distance-Based Learning)

Classifies data based on similarity to nearby points in feature space.Compare performance with tree-based models but struggles with high-dimensional data.


```{python}
from sklearn.neighbors import KNeighborsClassifier  


knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')


knn_model.fit(x_train_scaled, y_train)

knn_y_pred = knn_model.predict(x_test_scaled)


print("KNN Classification Report:")
print(classification_report(y_test, knn_y_pred))
print("KNN Accuracy:", accuracy_score(y_test, knn_y_pred))
```


## 5.3 Model-2 : Random Forest Classifier(Ensemble Learning)

Builds multiple decision trees and averages their predictions for stability.Handles large feature sets well and provides high accuracy.



```{python}
import warnings
from sklearn.exceptions import DataConversionWarning
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score  

warnings.filterwarnings(action='ignore', category=DataConversionWarning)


rf_model = RandomForestClassifier(n_estimators=1000, random_state=42)


rf_model.fit(x_train_scaled, y_train)


rf_y_pred = rf_model.predict(x_test_scaled)



print("Random Forest Classification Report:")
print(classification_report(y_test, rf_y_pred))
print("Random Forest Accuracy:", accuracy_score(y_test, rf_y_pred))
```


## 5.4 - Model 3 : XGBoost(Gradient Boosting Algorithm)

Optimized boosting technique that corrects errors iteratively for higher accuracy.Handles imbalanced data better and is computationally efficient.


```{python}
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42)


xgb_model.fit(x_train_scaled, y_train)


xgb_y_pred = xgb_model.predict(x_test_scaled)


print("Accuracy:", accuracy_score(y_test, xgb_y_pred))
print("Classification Report:\n", classification_report(y_test, xgb_y_pred))


from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, xgb_y_pred)

# Plot confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=set(y_test['attack_cat']), yticklabels=set(y_test['attack_cat']))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - XGBoost Model')
plt.savefig('Confusion Matrix - XGBoost Model.png')
plt.show()
```




## 6. Evaluation Metrics


### Metrics used:

- **Accuracy**: Proportion of total correct predictions (both normal and attack) out of all predictions.
- **F1 Score**: Harmonic mean of precision and recall, balancing false positives and false negatives.
- **ROC AUC**: Measures model's ability to distinguish between classes across all thresholds (area under ROC curve).
- **AUPRC**: Area under the Precision-Recall curve, especially useful for imbalanced datasets like this one.
- **ROC Curve**: Plots True Positive Rate vs. False Positive Rate to visualize classification performance.



```{python}
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

from sklearn.preprocessing import label_binarize


y_test_binarized = label_binarize(y_test, classes=list(range(len(y_test['attack_cat'].unique()))))


rf_y_probs = rf_model.predict_proba(x_test_scaled)  # Probabilities for all classes


fpr = {}
tpr = {}
roc_auc = {}
for i in range(y_test_binarized.shape[1]):
	fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], rf_y_probs[:, i])
	roc_auc[i] = auc(fpr[i], tpr[i])


precision = {}
recall = {}
auprc = {}
for i in range(y_test_binarized.shape[1]):
	precision[i], recall[i], _ = precision_recall_curve(y_test_binarized[:, i], rf_y_probs[:, i])
	auprc[i] = average_precision_score(y_test_binarized[:, i], rf_y_probs[:, i])


plt.figure(figsize=(8, 6))
for i in range(y_test_binarized.shape[1]):
	plt.plot(fpr[i], tpr[i], lw=2, label=f'Class {i} (AUC = {roc_auc[i]:.3f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random Guess Line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Random Forest ROC Curve')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


plt.figure(figsize=(8, 6))
for i in range(y_test_binarized.shape[1]):
	plt.plot(recall[i], precision[i], lw=2, label=f'Class {i} (AUPRC = {auprc[i]:.3f})')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Random Forest Precision-Recall Curve')
plt.legend(loc="upper right")
plt.grid(True)
plt.show()


for i in range(y_test_binarized.shape[1]):
	print(f"Class {i} AUC Score: {roc_auc[i]:.3f}")
	print(f"Class {i} AUPRC Score: {auprc[i]:.3f}")

```



```{python}
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score

from sklearn.preprocessing import label_binarize


y_test_binarized = label_binarize(y_test, classes=list(range(len(y_test['attack_cat'].unique()))))


rf_y_probs = rf_model.predict_proba(x_test_scaled)  # Random Forest
xgb_y_probs = xgb_model.predict_proba(x_test_scaled)  # XGBoost


try:
    knn_y_probs = knn_model.predict_proba(x_test_scaled)  # KNN
except:
    knn_y_probs = label_binarize(knn_model.predict(x_test_scaled), classes=list(range(len(y_test['attack_cat'].unique()))))


models = {"Random Forest": rf_y_probs, "KNN": knn_y_probs, "XGBoost": xgb_y_probs}
roc_results = {}

plt.figure(figsize=(10, 7))

for model_name, y_probs in models.items():
    for i in range(y_test_binarized.shape[1]):  # Loop through each class
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_probs[:, i])
        roc_auc = auc(fpr, tpr)
        roc_results[f"{model_name} (Class {i})"] = roc_auc
        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (Class {i}, AUC = {roc_auc:.3f})')

# Plot Random Guess Line
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison for All Models')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


auprc_results = {}

plt.figure(figsize=(10, 7))

for model_name, y_probs in models.items():
    for i in range(y_test_binarized.shape[1]):  # Loop through each class
        precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_probs[:, i])
        auprc = average_precision_score(y_test_binarized[:, i], y_probs[:, i])
        auprc_results[f"{model_name} (Class {i})"] = auprc
        plt.plot(recall, precision, lw=2, label=f'{model_name} (Class {i}, AUPRC = {auprc:.3f})')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve Comparison for All Models')
plt.legend(loc="upper right")
plt.grid(True)
plt.show()


print("\nðŸš€ Model Performance Comparison (AUC & AUPRC Scores):")
for model in models.keys():
    for i in range(y_test_binarized.shape[1]):
        print(f"{model} (Class {i}) - AUC: {roc_results[f'{model} (Class {i})']:.3f}, AUPRC: {auprc_results[f'{model} (Class {i})']:.3f}")

```


```{python}
import matplotlib.pyplot as plt
import numpy as np


models = ["Random Forest", "KNN", "XGBoost"]
classes = [f"Class {i}" for i in range(11)]

# Extract AUC Scores
rf_auc = [0.904, 0.892, 0.902, 0.879, 0.951, 0.963, 0.979, 0.997, 0.963, 0.991, 0.998]
knn_auc = [0.882, 0.860, 0.603, 0.753, 0.908, 0.924, 0.961, 0.995, 0.917, 0.968, 0.992]
xgb_auc = [0.905, 0.886, 0.893, 0.865, 0.951, 0.962, 0.977, 0.998, 0.967, 0.986, 0.997]

# Extract AUPRC Scores
rf_auprc = [0.431, 0.411, 0.420, 0.566, 0.802, 0.861, 0.936, 0.994, 0.869, 0.921, 0.987]
knn_auprc = [0.377, 0.328, 0.220, 0.421, 0.690, 0.776, 0.897, 0.989, 0.824, 0.848, 0.959]
xgb_auprc = [0.430, 0.373, 0.369, 0.523, 0.777, 0.829, 0.927, 0.994, 0.874, 0.867, 0.971]

# Plot AUC Comparison
plt.figure(figsize=(12, 6))
x = np.arange(len(classes))
width = 0.2  # Bar width

plt.bar(x - width, rf_auc, width=width, label="Random Forest AUC", color="blue")
plt.bar(x, knn_auc, width=width, label="KNN AUC", color="red")
plt.bar(x + width, xgb_auc, width=width, label="XGBoost AUC", color="yellow")

plt.xlabel("Attack Classes")
plt.ylabel("AUC Score")
plt.title("AUC Score Comparison Across Models")
plt.xticks(ticks=x, labels=classes, rotation=45)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

# Plot AUPRC Comparison
plt.figure(figsize=(12, 6))

plt.bar(x - width, rf_auprc, width=width, label="Random Forest AUPRC", color="blue")
plt.bar(x, knn_auprc, width=width, label="KNN AUPRC", color="red")
plt.bar(x + width, xgb_auprc, width=width, label="XGBoost AUPRC", color="green")

plt.xlabel("Attack Classes")
plt.ylabel("AUPRC Score")
plt.title("AUPRC Score Comparison Across Models")
plt.xticks(ticks=x, labels=classes, rotation=45)
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.show()

```



## Impact {.unnumbered}


Through comprehensive modeling and evaluation, the XGBoost classifier emerged as the best-performing model, consistently delivering high accuracy, F1-score, ROC AUC, and AUPRC, especially on the imbalanced UNSW-NB15 dataset. While Logistic Regression offers strong interpretability, and Random Forest achieved competitive performance, XGBoost demonstrated the best trade-off between precision and recall. Our use of mutual information for feature selection, along with resampling techniques and metric-based evaluation, significantly improved model robustness. This analysis highlights the critical role of feature engineering, model tuning, and metric selection in building effective intrusion detection systems capable of identifying cyber threats in real-world network traffic.